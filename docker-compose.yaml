version: "3"
services:
  # llama.cpp:
  #   command:
  #     [
  #       "--ctx-size",
  #       "$MODEL_CTX_SIZE",
  #       "--host",
  #       "0.0.0.0",
  #       "--mlock",
  #       "--model",
  #       "/models/$MODEL_FILE_NAME",
  #       "--port",
  #       "$MODEL_PORT",
  #     ]
  #   entrypoint: ["/app/server"]
  #   env_file:
  #     - .env
  #   image: ghcr.io/ggerganov/llama.cpp:full
  #   volumes:
  #     - ./models:/models
  app:
    build:
      # args:
      #   - TAG=${TAG}
      dockerfile: Dockerfile
      # pull: ${PULL_SOURCE_IMAGE}
      # tags:
      #   - ${PROJECT_NAME}:${TAG}
      #   - ${PROJECT_NAME}:latest
    command: ["sleep", "infinity"]
    ports:
      - 3000:3000
    volumes:
      - ./app/src:/app/app/src
      - ./frontend_developer_agent:/app/frontend_developer_agent
